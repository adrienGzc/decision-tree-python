{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report lab 2: Decision Tree.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The main goal of this asignment was to create a decision tree from scratch for classification task.\n",
    "Before to rush in the code, I started to make some research of different kind of decision tree implementation.\n",
    "I found different implementation, like CART or ID3. I start to implement both on different code base but I stick with an alike CART implementation (whithout the regression task).\n",
    "I mainly looked on wikipedia website for information: [Decision tree](https://en.wikipedia.org/wiki/Decision_tree_learning), [ID3 Decsion tree](https://en.wikipedia.org/wiki/ID3_algorithm).\n",
    "All the splitting part was, at the beginning, confusing for me but I also read this [article](https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052) which helped me a lot.\n",
    "\n",
    "The code is available online on my Github account through this [link](https://github.com/adrienGzc/decision-tree-python).\n",
    "\n",
    "\n",
    "## Data Loader\n",
    "At first, I create my own data loader. As it was indicate to test our decision tree with multiple dataset I simplify the process. Also because it's going to be helpful in other assignment.\n",
    "\n",
    "Here is my DataLoader class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "class Loader:\n",
    "  def __init__(self):\n",
    "    self.dataLoader = {\n",
    "      'iris': datasets.load_iris,\n",
    "      'wine': datasets.load_wine,\n",
    "      'cancer': datasets.load_breast_cancer,\n",
    "    }\n",
    "\n",
    "  def getDataset(self, name):\n",
    "    if name not in self.dataLoader:\n",
    "      print('Error: the only dataset available are: ', [key for key in self.dataLoader])\n",
    "      return(84)\n",
    "    return self.dataLoader[name]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see you are limited for the type of data possible but it can grow further as I need specific dataset.\n",
    "\n",
    "\n",
    "## Cross Validator\n",
    "This wasn't required specificly in the assignment but I wanted my decision tree compliant with the cross validator that we had to made in assignment 1.\n",
    "My cross validator wasn't that generic as I thought so I had to change the extract function, used for extract the label of the instance in the dataset, to make it work. I used the same as I used in the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "class CrossValidator:\n",
    "  def __init__(self, algo=None, dataset=None, nbFolds=10):\n",
    "    random.seed(1)\n",
    "    self.folds = list()\n",
    "    self.algorithm = algo\n",
    "    self.dataset = self.__transformDataIntoList(dataset)\n",
    "    self.nbFolds = nbFolds\n",
    "    self.rocData = list()\n",
    "\n",
    "  def __transformDataIntoList(self, data):\n",
    "    if data is not None:\n",
    "      return [list(instance) for instance in data]\n",
    "    return None\n",
    "\n",
    "  # Method to check if the CrossValidator class as everything needed to start.\n",
    "  def __checkNotEmptyAttributes(self):\n",
    "    if (self.algorithm is None or self.dataset is None or self.nbFolds <= 1):\n",
    "      print(\"Error: Algorithm and dataset shouldn't be empty and nbFolds neither less nor equal to 0\")\n",
    "      return False\n",
    "    return True\n",
    "\n",
    "  # nbInstances as to be lower than nbFolds, I round the return to get a integer and not a float.\n",
    "  def __getFoldSize(self, nbInstances, nbFolds):\n",
    "    return round(nbInstances / nbFolds)\n",
    "\n",
    "  # Fill the folds Class variable with all folds of instances shuffled.\n",
    "  def __splitDatasetIntoKFolds(self):\n",
    "    copyDataset = self.dataset.copy()\n",
    "    # Desorganize the dataset\n",
    "    random.shuffle(copyDataset)\n",
    "    # Get the number of instances in each fold.\n",
    "    foldSize = self.__getFoldSize(len(self.dataset), self.nbFolds)\n",
    "\n",
    "    # I move the pointer start and end to cur the dataset into the number of instances calculated.\n",
    "    for nb in range(self.nbFolds):\n",
    "      start = foldSize * nb\n",
    "      end = foldSize + start\n",
    "      self.folds.append(copyDataset[start:end])\n",
    "\n",
    "  # Return the taget label from the dataset, target as to be at the end.\n",
    "  def __extractTargetFromDataset(self, data):\n",
    "    newDataset = list()\n",
    "    target = list()\n",
    "    for instance in data:\n",
    "      target.append(instance[len(instance) - 1])\n",
    "      newDataset.append(list(instance[:-1][0]))\n",
    "    return newDataset, target\n",
    "\n",
    "  # Count the correct answer and return the accuracy of them, scaled on 0 to 100%.\n",
    "  def __getAccuracy(self, original, predictions):\n",
    "    nbCorrectPredictions = 0\n",
    "\n",
    "    # Loop through all the predictions.\n",
    "    for index in range(len(predictions)):\n",
    "      # Get the class predicted.\n",
    "      predictClass = predictions[index][0]\n",
    "\n",
    "      # If she correspond to the target label then add a correct answer.\n",
    "      if (original[index] == predictClass):\n",
    "        nbCorrectPredictions += 1\n",
    "\n",
    "    return nbCorrectPredictions / len(original) * 100\n",
    "\n",
    "  # Return a simple list of instances as a deep copy and delete the testing fold.\n",
    "  def __getTrainData(self, dataToSquash, indexToRemove):\n",
    "    data = copy.deepcopy(self.folds)\n",
    "    data.pop(indexToRemove)\n",
    "    return sum(data, [])\n",
    "\n",
    "  # Some magic here. Add the target label to the prediction information for the ROC.\n",
    "  def __appendTargetToPrediction(self, targets, predictions):\n",
    "    for index in range(len(predictions)):\n",
    "      tmp = list(predictions[index])\n",
    "      tmp.append(targets[index])\n",
    "      predictions[index] = tmp\n",
    "    return predictions\n",
    "\n",
    "  # Calculate the score of the accuracy:\n",
    "  #   - all the accuracy as a list, len(list accuracy) = nbFolds.\n",
    "  #   - the mean accuracy based on all the accuracy.\n",
    "  #   - a list with -> Prediction, Classes probabilities, Real target expected.\n",
    "  def score(self):\n",
    "    if (self.__checkNotEmptyAttributes() is False):\n",
    "      return False\n",
    "\n",
    "    # Split the data into K folds.\n",
    "    self.__splitDatasetIntoKFolds()\n",
    "    accuracyScores = list()\n",
    "    for index, fold in enumerate(self.folds):\n",
    "      trainData = self.__getTrainData(self.folds, index)\n",
    "      # Extract the label from the dataset.\n",
    "      X_train, targetTrain = self.__extractTargetFromDataset(trainData)\n",
    "      testData = copy.deepcopy(fold)\n",
    "      X_test, targetTest = self.__extractTargetFromDataset(testData)\n",
    "\n",
    "      # Train the Naive Bayes algorithm.\n",
    "      self.algorithm.fit(X_train, targetTrain)\n",
    "      # Predict with the fold.\n",
    "      acc, predictionFold = self.algorithm.predict(X_test, targetTest)\n",
    "      # Add to the data for the ROC the prediction information with the target label.\n",
    "      self.rocData.extend(self.__appendTargetToPrediction(targetTest, predictionFold))\n",
    "      # Add the accuracy calculate.\n",
    "      accuracyScores.append(acc)\n",
    "    return accuracyScores, sum(accuracyScores) / len(accuracyScores), self.rocData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not gonna re-explain it because it is the same as in the assignment 2, but if you need more understanding I added a lot of comment in the code to explain what I did an how it works. You could also check on the assignment 1 report to get more information about it.\n",
    "\n",
    "\n",
    "## Decision Tree\n",
    "Before to go any further and after my research about decision trees, I found important to have the same way of use for all algorithm. In assignment 1, I took inspiration from sklearn with the way to use the different algorithm. They all have the same function to train and to predict. So, as it was important for my cross validator to have the same function, I used the same standard from my naive bayes.\n",
    "\n",
    "The function to train is `fit()` and take 2 parameters:\n",
    "1 - Dataset (mandatory)\n",
    "2 - Label for each instance in param 1 (mandatory)\n",
    "\n",
    "The function for prediction is `predict()` and take 2 parameters as well:\n",
    "1 - Dataset (mandatory)\n",
    "2 - Label for each instance in param 1 (optional)\n",
    "If no label provided then return a list with all prediction made for each instance. It's up to the user to check and get the accuracy after prediction in this case. Otherwise, the accuracy is also returned with the list.\n",
    "\n",
    "For the development I also add a function to display the tree created, I thought this could be usefull so I let the function in it. Even though, the display might be confusing to understand at the beginning.\n",
    "\n",
    "At the creation of the decision tree you can precise if you want a max depth for the tree. I add this parameter which I found usefull if you have a huge number of attributes in your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pow\n",
    "\n",
    "class DecisionTree:\n",
    "  def __init__(self, maxDepth=None):\n",
    "    self.__tree = None\n",
    "    self.__maxDepth = maxDepth\n",
    "  \n",
    "  def __transformDataIntoList(self, data):\n",
    "    return [list(instance) for instance in data]\n",
    "\n",
    "  # Return the taget label from the dataset, target as to be at the end.\n",
    "  def __extractTargetFromDataset(self, data):\n",
    "    newDataset = list()\n",
    "    target = list()\n",
    "    for instance in data:\n",
    "      target.append(instance[len(instance) - 1])\n",
    "      newDataset.append(instance[:-1])\n",
    "    return newDataset, target\n",
    "\n",
    "  # Add target label to the dataset (add 1 column).\n",
    "  def __concateTargetWithDataset(self, dataset, targetDataset):\n",
    "    data = list()\n",
    "    for index, instance in enumerate(dataset):\n",
    "      tmp = list()\n",
    "      if type(instance) is not list:\n",
    "        print('NOT', instance)\n",
    "        tmp.append(instance)\n",
    "      else:\n",
    "        tmp = list(instance)\n",
    "      tmp.append(targetDataset[index])\n",
    "      data.append(tmp)\n",
    "    return data\n",
    "\n",
    "  def __countUniqueValue(self, data):\n",
    "    return list(set(data))\n",
    "\n",
    "  # Return a dict with all the classes as key and the nb of each class as value.\n",
    "  def __countAllElemInList(self, listElem):\n",
    "    nbAllElem = dict()\n",
    "\n",
    "    for elem in listElem:\n",
    "      if elem not in nbAllElem:\n",
    "        nbAllElem[elem] = 1\n",
    "      else:\n",
    "        nbAllElem[elem] += 1\n",
    "    return nbAllElem\n",
    "\n",
    "  def __calculateGiniScore(self, leafs):\n",
    "    # Get size of all instance in each leaf.\n",
    "    nbInstances = sum([len(leaf) for leaf in leafs])\n",
    "    giniScore = list()\n",
    "\n",
    "    for leaf in leafs:\n",
    "      data, target = self.__extractTargetFromDataset(leaf)\n",
    "      # Get size of the leaf and if 0 then return 0 since there is no data.\n",
    "      sizeLeaf = len(data)\n",
    "      if sizeLeaf == 0: continue\n",
    "      # Count all instance depending on each classes.\n",
    "      nbClassElem = self.__countAllElemInList(target)\n",
    "      # Add the score for each class together.\n",
    "      classScore = sum([pow((val / sizeLeaf), 2) for val in nbClassElem.values()])\n",
    "      giniScore.append((1.0 - classScore) * (sizeLeaf / nbInstances))\n",
    "    return sum(giniScore)\n",
    "\n",
    "  # Return the 2 leaf containing the splitted data on the breakpoint.\n",
    "  def __split(self, data, indexAttr, splitValue):\n",
    "    leftLeaf = list()\n",
    "    rightLeaf = list()\n",
    "\n",
    "    # According to the subject, lower value to the left and rest at the right.\n",
    "    for instance in data:\n",
    "      # print(instance[indexAttr], splitValue)\n",
    "      if instance[indexAttr] < splitValue:\n",
    "        leftLeaf.append(instance)\n",
    "      else:\n",
    "        rightLeaf.append(instance)\n",
    "    return leftLeaf, rightLeaf\n",
    "\n",
    "  # Return a dict for the best split node found.\n",
    "  def __foundBestSplit(self, dataset):\n",
    "    # Detached the target label from the dataset.\n",
    "    data, _target = self.__extractTargetFromDataset(dataset)\n",
    "    tree = dict()\n",
    "    indexAttr = 0\n",
    "\n",
    "    # Loop through each attribute, zip return all the column at once.\n",
    "    for attribute in zip(*data):\n",
    "      for value in attribute:\n",
    "        # Get the two leaf for split (left and right leafs).\n",
    "        leftLeaf, rightLeaf = self.__split(dataset, indexAttr, value)\n",
    "        # Calculate gini scrore for value as breakpoint.\n",
    "        giniScore = self.__calculateGiniScore((leftLeaf, rightLeaf, ))\n",
    "        if not tree or tree['gini'] > giniScore:\n",
    "          tree = {'breakpoint': value, 'indexAttr': indexAttr, 'leftLeaf': leftLeaf, 'rightLeaf': rightLeaf, 'gini': giniScore}\n",
    "      indexAttr += 1\n",
    "    return tree\n",
    "\n",
    "  def __getResult(self, leafs):\n",
    "    # Extract the target from the leafs to get the result.\n",
    "    # If multiple target then take the highest one.\n",
    "    _data, target = self.__extractTargetFromDataset(leafs)\n",
    "    return max(self.__countUniqueValue(target))\n",
    "\n",
    "  def __recursiveCreation(self, tree, depth, maxDepth):\n",
    "    # Check empty data in split.\n",
    "    if not tree['leftLeaf'] or not tree['rightLeaf']:\n",
    "      joinLeaf = tree['leftLeaf'] + tree['rightLeaf']\n",
    "      tree['leftLeaf'] = self.__getResult(joinLeaf)\n",
    "      tree['rightLeaf'] = self.__getResult(joinLeaf)\n",
    "      return\n",
    "    elif maxDepth is not None and maxDepth >= depth:\n",
    "      # If maxDepth set then check if value is reach.\n",
    "      tree['leftLeaf'] = self.__getResult(tree['leftLeaf'])\n",
    "      tree['rightLeaf'] = self.__getResult(tree['rightLeaf'])\n",
    "      return\n",
    "    \n",
    "    # Split left\n",
    "    tree['leftLeaf'] = self.__foundBestSplit(tree['leftLeaf'])\n",
    "    self.__recursiveCreation(tree['leftLeaf'], depth + 1, maxDepth)\n",
    "\n",
    "    # Split right\n",
    "    tree['rightLeaf'] = self.__foundBestSplit(tree['rightLeaf'])\n",
    "    self.__recursiveCreation(tree['rightLeaf'], depth + 1, maxDepth)\n",
    "\n",
    "  def __createTree(self, dataset):\n",
    "    # Create root node of the tree.\n",
    "    self.__tree = self.__foundBestSplit(dataset)\n",
    "    # Create rest of the tree.\n",
    "    self.__recursiveCreation(self.__tree, 0, self.__maxDepth)\n",
    "\n",
    "  # Function to train and create a decision tree.\n",
    "  def fit(self, dataset, target):\n",
    "    # Get the root of the tree at first.\n",
    "    dataset = self.__transformDataIntoList(dataset)\n",
    "    # Start creating the leaf of the tree with the split.\n",
    "    # print(dataset, target)\n",
    "    dataset = self.__concateTargetWithDataset(dataset, target)\n",
    "    self.__createTree(dataset)\n",
    "\n",
    "  def __getAccurary(self, predictions):\n",
    "    nbPredictions = len(predictions)\n",
    "    counter = 0\n",
    "\n",
    "    for predict in predictions:\n",
    "      if predict[0] == predict[1]:\n",
    "        counter += 1\n",
    "    return round(counter / nbPredictions, 2)\n",
    "\n",
    "  # Make recursive prediction through the all tree.\n",
    "  def __makePrediction(self, instance, tree):\n",
    "    # If attribute of the instance is lower than the breakpoint found in the training\n",
    "    # then go to the right of the tree.\n",
    "    if instance[tree['indexAttr']] > tree['breakpoint']:\n",
    "      # Check if the right is a leaf or a endpoint.\n",
    "      if isinstance(tree['rightLeaf'], dict):\n",
    "        return self.__makePrediction(instance, tree['rightLeaf'])\n",
    "      # If not a real leaf then return the result branch\n",
    "      return tree['rightLeaf']\n",
    "    else:\n",
    "      # Doing exactly the same but for the left branch of the tree.\n",
    "      if isinstance(tree['leftLeaf'], dict):\n",
    "        return self.__makePrediction(instance, tree['leftLeaf'])\n",
    "      # If not a real leaf then return the result branch\n",
    "      return tree['leftLeaf']\n",
    "\n",
    "  # Make prediction on an instance or a list.\n",
    "  # Return a list of Tuple as (TARGET, PREDICTION).\n",
    "  # If no target provide then return list of predictions.\n",
    "  def predict(self, dataset, target=None):\n",
    "    if self.__tree is None:\n",
    "      print('Error: You need to fit the decision tree first with fit(dataset, target).')\n",
    "      return 84\n",
    "    \n",
    "    predictions = list()\n",
    "    # Check for one row only prediction.\n",
    "    if len(dataset) == 1:\n",
    "      self.__makePrediction(dataset, self.__tree)\n",
    "    else:\n",
    "      # Otherwise iterate through the all dataset and make a prediction for each instance.\n",
    "      for index, instance in enumerate(dataset):\n",
    "        result = self.__makePrediction(instance, self.__tree)\n",
    "        if target is None:\n",
    "          predictions.append(result)\n",
    "        else:\n",
    "          predictions.append((target[index], result, ))\n",
    "      accuracy = self.__getAccurary(predictions)\n",
    "      return accuracy, predictions\n",
    "\n",
    "  # Simple recursion function to display the tree trained.\n",
    "  def __displayTree(self, tree, depth, label='root'):\n",
    "    sentence = ''\n",
    "    if isinstance(tree, dict):\n",
    "      for _space in range(0, depth):\n",
    "        sentence += ' '\n",
    "      sentence += '%s -> X%d, value < %.3f, gini: %.3f' % (label, tree['indexAttr'] + 1, tree['breakpoint'], tree['gini'])\n",
    "      print(sentence)\n",
    "      self.__displayTree(tree['leftLeaf'], depth + 1, 'left')\n",
    "      self.__displayTree(tree['rightLeaf'], depth + 1, 'right')\n",
    "    else:\n",
    "      for _space in range(0, depth):\n",
    "        sentence += ' '\n",
    "      print(sentence, tree)\n",
    "\n",
    "  # Function to call to display the training tree result.\n",
    "  def show(self):\n",
    "    if self.__tree is not None:\n",
    "      self.__displayTree(self.__tree, 0)\n",
    "    else:\n",
    "      print('Error: No present tree. You need to fit the DecistionTree first.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I force myself to make the function name and even the variable name the more clear possible to understand at the first lecture of the code. Concerne that it can be confusing, I also add lot of comments of each step in the code.\n",
    "\n",
    "For the training, I used the gini function instead of the entropy in the dataset. Even if they provide the same kind of information which is the impurity of the given dataset. I found the gini easier to calculate and implement, that's why I started with this one. An implementation of the entropy with the information gain is ongoing but unfortunetly I missing time to delivered the working code for this part. Stay tuned on my [Github repository](https://github.com/adrienGzc/decision-tree-python) to check the update.\n",
    "\n",
    "\n",
    "## Main\n",
    "Here you will find the 4 main function called when you called the `main.py` file.\n",
    "Why 4 different main? Because I tested my desicion tree with 3 different dataset: Iris, Wine and Breast cancer.\n",
    "The iris is tested whithout cross validation but the wine and breast cancer are tested with cross validation.\n",
    "If you gonna run the script, the breast cancer dataset is quite long to compute so just be patient.\n",
    "\n",
    "I actually got 6 point ish difference with sklearn decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Wine\n",
      "Accurary: 0.87%\n",
      "\n",
      "Dataset: Iris\n",
      "Accuracy: 0.89%\n",
      "\n",
      "Dataset: Breast cancer\n",
      "Accuracy: 0.93%\n",
      "\n",
      "SKLEARN\n",
      "Dataset: Iris\n",
      "Accuracy: 0.93%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from modules.Loader import Loader\n",
    "from modules.CrossValidator import CrossValidator\n",
    "from modules.DecisionTree import DecisionTree\n",
    "\n",
    "loader = Loader()\n",
    "\n",
    "# Add target label to the dataset (add 1 column).\n",
    "def concateTargetWithDataset(dataset, targetDataset):\n",
    "  data = list()\n",
    "  for index, instance in enumerate(dataset):\n",
    "    tmp = list()\n",
    "    if type(instance) is not list:\n",
    "      tmp.append(instance)\n",
    "    else:\n",
    "      tmp = list(instance)\n",
    "    tmp.append(targetDataset[index])\n",
    "    data.append(tmp)\n",
    "  return data\n",
    "\n",
    "def mainBreastCancer():\n",
    "  cancerData = loader.getDataset('cancer')\n",
    "  cancerData = concateTargetWithDataset(cancerData['data'], cancerData['target'])\n",
    "  decisionTree = DecisionTree()\n",
    "  crossValidator = CrossValidator(algo=decisionTree, dataset=cancerData, nbFolds=10)\n",
    "  _scoresByFold, meanAccuracy, _rocData = crossValidator.score()\n",
    "  print('Dataset: Breast cancer\\nAccuracy: %.2f%%\\n' % meanAccuracy)\n",
    "\n",
    "def mainSklearn():\n",
    "  irisData = loader.getDataset('iris')\n",
    "  X_train, X_test, y_train, y_test = train_test_split(irisData['data'], irisData['target'], test_size=0.80, random_state=42)\n",
    "  clf = tree.DecisionTreeClassifier()\n",
    "  clf = clf.fit(X_train, y_train)\n",
    "  print('SKLEARN\\nDataset: Iris\\nAccuracy: %.2f%%\\n' % clf.score(X_test, y_test))\n",
    "\n",
    "def crossValTest():\n",
    "  irisData = loader.getDataset('iris')\n",
    "  irisData = concateTargetWithDataset(irisData['data'], irisData['target'])\n",
    "  decisionTree = DecisionTree()\n",
    "  crossValidator = CrossValidator(algo=decisionTree, dataset=irisData, nbFolds=10)\n",
    "  _scoresByFold, meanAccuracy, _rocData = crossValidator.score()\n",
    "  print('Dataset: Iris\\nAccuracy: %.2f%%\\n' % meanAccuracy)\n",
    "\n",
    "def main():\n",
    "  wineData = loader.getDataset('wine')\n",
    "  X_train, X_test, y_train, y_test = train_test_split(wineData['data'], wineData['target'], test_size=0.80, random_state=42)\n",
    "  decisionTree = DecisionTree()\n",
    "  decisionTree.fit(X_train, y_train)\n",
    "  acc, _predictions = decisionTree.predict(X_test, y_test)\n",
    "  print('Dataset: Wine\\nAccurary: %.2f%%\\n' % acc)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    crossValTest()\n",
    "    mainBreastCancer()\n",
    "    mainSklearn()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
